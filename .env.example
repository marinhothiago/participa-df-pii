
###############################################
# Exemplo de .env para Participa DF Backend   #
###############################################

# Token de autenticação Hugging Face (OBRIGATÓRIO para LLAMA Árbitro funcionar)
# Obtenha em: https://huggingface.co/settings/tokens
# O token precisa ter acesso ao modelo (aceitar termos de uso do Llama)
HF_TOKEN=seu_token_huggingface_aqui

# Modelo LLM para arbitragem (opcional - usa Llama-3.2-3B por padrão)
# Modelos disponíveis: meta-llama/Llama-3.2-3B-Instruct, meta-llama/Llama-3.1-70B-Instruct
# HF_MODEL=meta-llama/Llama-3.2-3B-Instruct

# Configuração do LLAMA Árbitro (ativado por padrão v9.5.0)
# O árbitro é acionado automaticamente em casos ambíguos de PII
PII_USE_LLM_ARBITRATION=True

# Usar GPU para modelos NER (se disponível)
PII_USAR_GPU=True

# Configuração do Celery/Redis (processamento em lote)
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0

# Instruções:
# 1. Renomeie este arquivo para .env
# 2. Obtenha HF_TOKEN em https://huggingface.co/settings/tokens
# 3. Aceite os termos de uso do Llama em huggingface.co/meta-llama
# 4. O sistema carrega automaticamente o .env em todos os entrypoints do backend
# 5. Para desativar LLAMA: PII_USE_LLM_ARBITRATION=False
# 6. Sem HF_TOKEN válido, o sistema usa fallback local (ensemble sem árbitro LLM)